# LLM-RAG

This project demonstrates Retrieval-Augmented Generation (RAG) using lightweight tools (```FAISS + all-MiniLM-L6-v2 + flan-t5-small```) to enhance LLMs with external knowledge. Designed for Kaggle/CPU environments, it:

1. Embeds documents into a vector database for fast retrieval
2. Generates answers using retrieved context
3. Balances accuracy and efficiency
4. Supports customization with domain-specific data

Outcome: Learn to integrate LLMs with dynamic knowledge sources while managing resource constraints.

